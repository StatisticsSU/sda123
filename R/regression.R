#' Summarize the results from a regression analysis
#'
#' Alternative to `summary.lm` to summarize a regression from `lm`.
#' Prints a table similar to the one generated by SAS and Minitab.
#' @param lmobject a fitted regression model from `lm`.
#' @param anova `TRUE` if an ANOVA table is computed.
#' @param fit_measures `TRUE` if measures of fit (RÂ² etc) is computed.
#' @param param `TRUE` if parameter estimates, standard errors etc is computed.
#' @param conf_intervals `TRUE` if confidence intervals for parameters.
#' @param vif_factors `TRUE` if variance inflation factors are to be printed.
#' @return list with three tables: param, anova and fit_measures
#' @export
#' @examples
#' library(sda123)
#' lmfit = lm(nRides ~ temp + hum + windspeed, data = bike)
#' regsumm = reg_summary(lmfit, anova = TRUE, conf_intervals = TRUE, vif_factors = TRUE)
#' regsumm$param
#' regsumm$anova
#' regsumm$fit_measures
reg_summary <- function(lmobject, anova = T,  fit_measures = T, param = T,
                       conf_intervals = F, vif_factors = F){

  if ("(Intercept)" %in% names(lmobject$coefficients)) intercept = 1 else intercept = 0

  lmsummary = summary(lmobject)
  df_regr = lmsummary$df[1] - intercept
  df_error = lmsummary$df[2]
  df_total = df_error + df_regr
  sse = (lmsummary$sigma^2)*df_error
  sst = var(lmobject$model[,1])*df_total
  ssr = sst - sse

  # Anova table
  anova_table = NA
  if (anova){
    anova_table = matrix(rep(NA,5*3),3,5)
    anova_table[,1] = c(df_regr,df_error,df_total)
    anova_table[,2] = c(ssr,sse,sst)
    anova_table[1:2,3] = c(ssr/df_regr, sse/df_error)
    anova_table[1,4] = lmsummary$fstatistic[1]
    anova_table[1,5] = pf(lmsummary$fstatistic[1], lmsummary$fstatistic[2], lmsummary$fstatistic[3], lower.tail = F)
    rownames(anova_table) <- c("Regr","Error","Total")
    colnames(anova_table) <- c("df","SS","MS","F","Pr(>F)")
    {cat("\nAnalysis of variance - ANOVA\n------------------------------------------------\n");
    print(anova_table, digits = 5, na.print = "")}
  }

  fit_table = NA
  if (fit_measures){
    fit_table = c(sqrt(sse/df_error), lmsummary$r.squared, lmsummary$adj.r.squared)
    names(fit_table) <- c("Root MSE","R2","R2-adj")
    {cat("\nMeasures of model fit\n------------------------------------------------\n");
    print(fit_table, digits = 5, na.print = "")}
  }

  # Table with estimated coefficients etc
  if (param){
    # Confidence intervals on parameters
    if (conf_intervals){
      param_table = cbind(lmsummary$coefficients, confint(lmobject))
    }else
    {
      param_table = lmsummary$coefficients
    }

    # Variance inflation factors
    if (vif_factors & df_regr>1){
      data = model.matrix(lmobject)
      X = data[,-1]
      vif = rep(NA,df_regr)
      for (j in 1:df_regr){
        vif[j] = 1/(1-summary(lm(X[,j] ~ data.matrix(X[,-j])))$r.squared)
      }
      if (intercept) vif = c(NA,vif)
      param_table = cbind(param_table,vif)
      colnames(param_table)[ncol(param_table)] = "VIF"
    }

    {cat("\nParameter estimates\n------------------------------------------------\n");
    print(param_table, digits = 5, na.print = "")}

  }else{param_table = NA}

  invisible(list(param = param_table, anova = anova_table, fit_measures = fit_table))
}

#' Plot confidence and prediction intervals for simple linear regression \cr
#'
#' @param formula an object of class "formula": a symbolic description of the model to be fitted.
#' @param data a data frame with the data.
#' @param level confidence level, default is level = 0.95
#' @param conf_int_line if TRUE, then conf intervals for regression line are plotted.
#' @param pred_interval if TRUE, then prediction intervals are plotted.
#' @return plot of data with overlayed intervals
#' @export
#' @examples
#' library(sda123)
#' reg_predict(mpg ~ hp, data = mtcars)
reg_predict <- function(formula, data, level = 0.95,
                             conf_int_line = T, pred_interval = T){

  conf_int_line = T # Bug if we don't plot these. TODO
  fit <- lm(formula, data = data)
  data <- cbind(data, suppressWarnings(
                  predict(fit, data = data, interval = "prediction",
                  level = level)))
  p <- ggplot(data, aes(x = .data[[names(fit$model)[2]]],
                        y = .data[[names(fit$model)[1]]])) +
    {if (pred_interval) geom_ribbon(aes(ymin = lwr, ymax = upr),
                          fill = prettycol[1], alpha = 0.3)
    } +
    {if (conf_int_line) suppressMessages(stat_smooth(method = "lm",
                          se = T, level = level,
                          color = prettycol[4], fill = prettycol[2]))
    else{geom_abline(aes(intercept = fit$coefficients[1],
                           slope = fit$coefficients[2]),
                           color = prettycol[2])}
    } +
    geom_point(color = prettycol[2], size = 1) +
    ggtitle("Konfidens- och prediktionsintervall") +
    theme_light() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
  return(p)
}

#' Simulate from a linear regression model
#'
#' Simulates a dataset with `n` observation from the linear regression model
#' \deqn{y = \beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k + \epsilon}{y = \beta_0 + \beta_1 * x_1 + ... + \beta_k * x_k + \epsilon, \epsilon ~ N(0, \sigma_ \epsilon^2)}
#' where the errors \eqn{\epsilon}{\epsilon} have zero mean and standard deviation \eqn{\sigma_ \epsilon}{\sigma_ \epsilon}, but can follow either normal or student-t distribution.
#' The variance can be homoscedastic or heteroscedastic with standard deviation function \eqn{\sigma_ \epsilon(x_1\gamma_1+\ldots+x_k \gamma_k)}{\sigma_ \epsilon(x_1\gamma_1+...+x_k \gamma_k)},
#' where the \eqn{(\gamma_1,\ldots,\gamma_k)}{(\gamma_1,...,\gamma_k)} vector of variance function parameters are given by the argument `heteroparams`. The \eqn{\epsilon}{\epsilon} can also have
#' an AR(1) autocorrelation structure with coefficient on first lag given by the argument ar1phi.
#' The covariates (x) are simulated from a normal distribution with the same correlation `rho_x`
#' between all pairs of covariates, and covariate \eqn{x_j}{x_j} has standard deviation `sigma_x[j]`.
#' Alternatively the covariate can follow a uniform distribution.
#' @param n the number of observations in the simulated dataset.
#' @param betavect a vector with regression coefficients
#' c(beta_0,beta_1,...beta_k). First element is intercept if `intercept = TRUE`
#' @param sigma_eps stdev of epsilon (homo) or
#' a variance function sigma_eps(X %*% heteroparams) with parameters heteroparams.
#' @param intercept if `TRUE` an intercept is added to the model.
#' @param responsedist options: `'normal'` or `'student'`
#' @param heteroparams parameters in the heteroscedastic variance function
#' @param studentdf degrees of freedom in the student-t errors
#' @param ar1phi AR(1) coefficient on first lag for autocorrelated errors
#' @param covdist distribution of the covariates. Options: `'normal'` or `'uniform'`.
#' @param rho_x correlation among the covariates. Same for all covariate pairs.
#' @param sigma_x vector with standard deviation of the covariates.
#' @return dataframe with simulated data (y, X1, X2, ..., XK) (no intercept included).
#' @export
#' @examples
#' library(sda123)
#' simdata <- reg_simulate(n = 500, betavect = c(1, -2, 1, 0), sigma_eps = 2)
#' lmfit <- lm(y ~ X1 + X2 + X3, data = simdata)
#' reg_summary(lmfit, anova = FALSE)
#'
#' # Simulate from a heteroscedastic student-t regression and detect problems with residuals
#' simdata <- reg_simulate(n = 500, betavect = c(1, -2, 1, 0), sigma_eps = exp, heteroparam = c(0,1,0,0), responsedist = 'student', studentdf = 4)
#' lmfit <- lm(y ~ X1 + X2 + X3, data = simdata)
#' reg_residuals(lmfit)
#'
#' #' # Simulate from a homoscedastic student-t regression with autocorrelated errors.
#' simdata <- reg_simulate(
#'  n = 500,
#'  betavect = c(1, -2, 1, 0),
#'  sigma_eps = 2,
#'  responsedist = 'student',
#'  studentdf = 4,
#'  ar1phi = 0.9
#' )
#' lmfit <- lm(y ~ X1 + X2 + X3, data = simdata)
#' reg_residuals(lmfit)
reg_simulate <- function(n, betavect, sigma_eps, intercept = TRUE, responsedist = 'normal',
                         heteroparams = NA, studentdf = NA, ar1phi = NA,
                         covdist = 'normal', rho_x = 0, sigma_x = rep(1,length(betavect)-intercept))
{
  k = length(betavect) - intercept

  # Generate covariates
  if (covdist == 'normal'){
    rho = matrix(rho_x, k, k)
    diag(rho) <- 1
    sigma = diag(sigma_x)%*%rho%*%diag(sigma_x)
    X = mvtnorm::rmvnorm(n, sigma = sigma)
  }else{
    X = matrix(runif(n*k), n, k)
    if (rho_x != 0) warning("uniformly distributed covariates are always uncorrelated")
  }
  if (intercept) X = cbind(1,X)

  # Possibly heteroscedastic variance
  if (is.function(sigma_eps)){
    message("heteroscedastic errors")
    if (is.na(heteroparams[1])) stop("need to specify argument heteroparams when sigma_eps is a function for heteroscedasticity")
    sigma_eps = sigma_eps(X %*% heteroparams) # sigma_eps is then a vector with stdevs
  }

  # Simulate epsilons
  if (responsedist != 'normal' && responsedist != 'student')   stop("responsedist must be 'normal' or 'student'")
  if (responsedist == 'normal'){epsilons = rnorm(n, sd = sigma_eps)}
  else{
      message("student-t errors")
      if (is.na(studentdf)) stop("Must specify dfstudent when using option responsedist == 'student")
      epsilons = rt(n, df = studentdf)*sigma_eps
  }
  # Making the epsilons autocorrelated with AR(1)
  if (!is.na(ar1phi)){epsilons = simAR1(n = length(epsilons), phi = ar1phi, sigma_eps = 1, epsilons = sqrt(1-ar1phi^2)*epsilons)}

  # Compute the final responses
  y = X%*%betavect + epsilons

  if (intercept) X = X[,-1] # remove intercept in the returned dataset
  data = data.frame(cbind(y,X))
  xnames = rep(NA,k)
  for (j in 1:k) xnames[j] = paste("X",j, sep = "")
  names(data) <- c("y", xnames)
  return(data)
}

#' K-fold cross-validation of regression models estimated with lm()
#'
#' @param formula an object of class "formula": a symbolic description of the model to be fitted.
#' @param data a data frame with the data used for fitting the models.
#' @param nfolds the number of folds in the cross-validation.
#' @param obs_order order of the observations when splitting the data. obs_order = "random" gives a random order.
#' @return RMSE Root mean squared prediction error on test data
#' @export
#' @examples
#' library(sda123)
#' RMSE_CV = reg_crossval(mpg ~ hp, data = mtcars, nfolds = 4, obs_order = 1:32)
#' print(RMSE_CV)
reg_crossval <- function(formula, data, nfolds, obs_order = "random"){

  n = dim(data)[1]
  if (is.character(obs_order)) obs_order = sample(1:n)

  obs_per_fold = ceiling(n/nfolds)
  yhat = matrix(NA, obs_per_fold, nfolds)
  if (n %% nfolds == 0){
    test_obs_matrix = matrix(obs_order, obs_per_fold) # k:th column contains test for fold k
  }else{
    nobs_last_fold = n-obs_per_fold*(nfolds-1)
    test_obs_matrix = matrix(NA, obs_per_fold, nfolds)
    test_obs_matrix[,1:(nfolds-1)] = matrix(obs_order[obs_per_fold*(nfolds-1)], obs_per_fold)
    test_obs_matrix[1:nobs_last_fold, nfolds] = obs_order[(obs_per_fold*(nfolds-1)+1):n]
  }
  for (k in 1:nfolds){
    testfold = test_obs_matrix[,k][!is.na(test_obs_matrix[,k])]
    trainingfold = setdiff(obs_order,testfold)
    fit = lm(formula, data = data[trainingfold,])
    yhat[1:length(testfold),k] = predict(fit, newdata = data[testfold,])
  }
  yhat = c(yhat)[!is.na(c(yhat))]
  yordered = data[all.names(formula)[2]][obs_order,]
  RMSE = sqrt(sum((yordered - yhat)^2)/n)
}

#' Residual analysis mimicing the 4-in-1 plots from Minitab
#'
#' Plots:
#' 1) Normal QQ-plot
#' 2) Residuals vs fitted values
#' 3) Histogram and normal density fit
#' 4) Residuals vs order.
#' @param lm_object a fitted regression model from `lm`.
#' @param studentized use (externally) studentized residuals. Defaults to FALSE.
#' @export
#' @examples
#' library(sda123)
#' fit = lm(mpg ~ hp, data = mtcars)
#' reg_residuals(fit)

reg_residuals <- function(lm_object, studentized = FALSE){

  n = dim(lm_object$model)[1]

  # adding residuals and fit to the data frame
  if (studentized) {
    lm_object$model$residuals <- rstudent(lm_object)
  }
  else {
    lm_object$model$residuals <- resid(lm_object)
  }
  
  lm_object$model$fitted <- fitted(lm_object)
  lm_object$model$obsnumber <- 1:n

  p1 = lm_object$model %>%
    ggplot(aes(sample = residuals)) +
    stat_qq_line(color = prettycol[4]) +
    stat_qq(color = prettycol[2], size = 1) +
    ylab("residuals") +
    theme_light()

  p2 = lm_object$model %>%
    ggplot(aes(x = fitted, y = residuals)) +
    geom_hline(aes(yintercept = 0), color = prettycol[4]) +
    geom_point(color = prettycol[2], size = 1) +
    xlab("fitted value") +
    ylab("residual") +
    theme_light() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())


  p3 = lm_object$model %>%
    ggplot(aes(x = residuals)) +
    geom_histogram(aes(y = after_stat(density)),
                   bins =  1 + ceiling(log(n,2)), fill = prettycol[1]) +
    stat_function(fun = dnorm, n = 101,
                  args = list(mean = 0, sd = sd(lm_object$model$residuals)),
                  color = prettycol[4]) +
    theme_light() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

  p4 = lm_object$model %>%
    ggplot(aes(x = obsnumber, y = residuals)) +
    geom_hline(aes(yintercept = 0), color = prettycol[4]) +
    geom_path(color = prettycol[1]) +
    geom_point(color = prettycol[2], size = 1)  +
    xlab("observation number") +
    theme_light() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())

  plot_grid(p1, p2, p3, p4)
}


#' Simulate from an AR(1) process
#'
#' Simulates `n` observations from
#' \deqn{x_t = \mu + \phi(x_{t-1}-\mu) + \epsilon, \epsilon \sim N(0, \sigma_\epsilon)}{x_t = \mu + \phi(x_{t-1}-\mu) + \epsilon, \epsilon ~ N(0, \sigma_ \epsilon^2)}
#' @param n Number of observations.
#' @param phi Value of the parameter phi. Phi = 0 will give white noise and phi = 1 a gaussian random walk.
#' @param mu Mean of the AR process.
#' @param sigma_eps Standard deviation of the error term.
#' @param epsilons Vector of error terms. If not included errors will be generated.
#' @export
#' @examples
#' library(sda123)
#' simdata = simAR1(n = 100, phi = 0.7, sigma_eps = 1)
#' plot(simdata)

simAR1 <- function(n, phi = 0.0, mu = 0, sigma_eps = 1, epsilons = NA){
  x = rep(mu, n) # start at steady state
  if (is.na(epsilons[1])){epsilons = rnorm(n, sd = sigma_eps)}
  for (t in 2:n){
    x[t] = mu + phi*(x[t-1] - mu) + epsilons[t]
  }
  return (x)
}




#' Summarize the results from a logistic regression analysis
#'
#' Alternative to `summary.glm` to summarize a regression from `glm`.
#' Prints a table similar to the one generated by SAS and Minitab.
#' @param glmobject a fitted regression model from `glm`.
#' @param odds_ratio `TRUE` if odds ratios for parameters is computed.
#' @param param `TRUE` if parameter estimates, standard errors etc is computed.
#' @param conf_intervals `TRUE` if confidence intervals for parameters.
#' @param digits Number of digits for the parameter estimates.
#' @return list with two tables: param, odds_ratio
#' @export
#' @examples
#' library(sda123)
#' glmfit <- glm(survived ~ age + sex + firstclass, data = titanic, family = binomial)
#' logisticreg_summary(glmfit)
logisticreg_summary <- function(glmobject, odds_ratio = T, param = T, conf_intervals = F, digits = 5){

  if ("(Intercept)" %in% names(glmobject$coefficients)) intercept = 1 else intercept = 0

  glmsummary = summary(glmobject)
  data = model.matrix(glmobject)
  X = data[,-1]
  k = ncol(X) # number of covariates, excluding intercept

  # Table with estimated coefficients etc
  if (param){
    # Confidence intervals on parameters
    if (conf_intervals){
      param_table = cbind(glmsummary$coefficients, suppressMessages(confint(glmobject)))
    }else
    {
      param_table = glmsummary$coefficients
    }

    {cat("\nParameter estimates\n------------------------------------------------\n");
      print(param_table, digits = digits, na.print = "")}

  }else{param_table = NA}



  # Table with odds ratios etc
  if (odds_ratio){
    # Confidence intervals on parameters
    if (conf_intervals){
      odds_ratio_table = cbind(exp(glmsummary$coef[,1:2]),glmsummary$coef[,3:4], exp(suppressMessages(confint.default(glmobject))))
    }else
    {
      odds_ratio_table = cbind(exp(glmsummary$coef[,1:2]),glmsummary$coef[,3:4])
    }

    {cat("\nOdds ratio estimates\n------------------------------------------------\n");
      print(odds_ratio_table, digits = digits, na.print = "")}

  }else{odds_ratio_table = NA}


  invisible(list(param = param_table, odds_ratio = odds_ratio_table))
}







#' Simulate from a logistic regression model
#'
#' Simulates a dataset with `n` observation from the logistic regression model \cr
#' \deqn{\mathrm{Pr}(y = 1 | x) = \frac{1}{1 + \exp(-(\beta_0 + \beta_1x_1 + \ldots + \beta_k x_k))}}{Pr(y = 1 | x) = 1/(1 + exp(-(\beta_0 + \beta_1 * x_1 + ... + \beta_k * x_k)))}
#' with covariates (x) simulated from a normal distribution with the same correlation `rho_x` \cr
#' between all pairs of covariates. Covariate x_j has standard deviation `sigma_x[j]`. \cr
#' Alternatively the covariate can follow a uniform distribution.
#' @param n the number of observations in the simulated dataset.
#' @param betavect a vector with regression coefficients
#' c(beta_0,beta_1,...beta_k). First element is intercept if `intercept = TRUE`
#' @param intercept if `TRUE` an intercept is added to the model.
#' @param covdist distribution of the covariates. Options: `'normal'` or `'uniform'`.
#' @param rho_x correlation among the covariates. Same for all covariate pairs.
#' @param sigma_x vector with standard deviation of the covariates.
#' @return dataframe with simulated data (y, X1, X2, ..., XK) (no intercept included).
#' @export
#' @examples
#' library(sda123)
#' simdata <- logisticreg_simulate(n = 500, betavect = c(1, -2, 1, 0))
#' glmfit <- glm(y ~ X1 + X2 + X3, data = simdata, family = binomial)
#' logisticreg_summary(glmfit, odds_ratio = FALSE)
logisticreg_simulate <- function(
    n,
    betavect,
    intercept = TRUE,
    covdist = 'normal',
    rho_x = 0,
    sigma_x = rep(1, length(betavect) - intercept)
  ){

  k = length(betavect) - intercept

  # Generate covariates
  if (covdist == 'normal'){
    rho = matrix(rho_x, k, k)
    diag(rho) <- 1
    sigma = diag(sigma_x)%*%rho%*%diag(sigma_x)
    X = mvtnorm::rmvnorm(n, sigma = sigma)
  }else{
    X = matrix(runif(n*k), n, k)
    if (rho_x != 0) warning("uniformly distributed covariates are always uncorrelated")
  }
  if (intercept) X = cbind(1,X)

  # Simulate binary responses
  y = rbinom(n, 1, prob = 1/(1 + exp(-X%*%betavect)))

  if (intercept) X = X[,-1] # remove intercept in the returned dataset
  data = data.frame(cbind(y,X))
  xnames = rep(NA,k)
  for (j in 1:k) xnames[j] = paste("X",j, sep = "")
  names(data) <- c("y", xnames)
  return(data)
}





